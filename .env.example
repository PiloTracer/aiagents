POSTGRES_DB=ragdb
POSTGRES_USER=raguser
POSTGRES_PASSWORD=ragpass
NEXT_PUBLIC_API_BASE=http://localhost:18001
CHAT_PROVIDER_BASE_URL=https://api.openai.com/v1

# Auth and keys
API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
AUTH_USERS="xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
AUTH_TOKEN_SECRET=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
AUTH_TOKEN_TTL_SECONDS=86400
OPENAI_ORG=org-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
OPENAI_PROJECT=proj_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Embeddings
EMBEDDING_MODEL=text-embedding-3-large
EMBEDDING_PROVIDER_BASE_URL=https://api.openai.com/v1
EMBEDDING_TARGET_DIM=3072
EMBEDDING_BATCH_SIZE=64
EMBEDDING_MAX_RETRIES=6
EMBEDDING_BACKOFF_BASE=1.0
LOCAL_EMBEDDING_MODEL=ai/mxbai-embed-large
#EMBEDDING_MODEL=text-embedding-3-large
LOCAL_EMBEDDING_BASE_URL=http://model-runner.docker.internal/engines/llama.cpp/v1
LOCAL_EMBEDDING_API_KEY=
LOCAL_EMBEDDING_TIMEOUT_SECONDS=120
DOCLING_VLM_MODEL=
LLAMA_MODEL_NAME=ai/llama3.1:8B-Q4_K_M
LLAMA_ENDPOINT=http://model-runner.docker.internal/engines/llama.cpp/v1

# Chat/completions
CHAT_MODEL=gpt-5-chat-latest
CHAT_TEMPERATURE=0.2
CHAT_TOP_P=1.0
CHAT_MAX_TOKENS=8192
#CHAT_MAX_TOKENS=1024
CHAT_MAX_RETRIES=5
CHAT_BACKOFF_BASE=0.6

# RAG tuning
MAX_CANDIDATE_CHUNKS=96
MAX_CHUNK_CHARS=1200
CHUNK_OVERLAP=250
TOP_K=24

# OCR and timeouts
TESS_LANGS="spa+eng"
HTTP_TIMEOUT_SECONDS=60

# CORS
ALLOWED_ORIGINS=*

# Provider selection defaults
LLM_PROVIDER=openai
EMBEDDING_PROVIDER=local
#EMBEDDING_PROVIDER=openai
ENABLE_PROVIDER_FALLBACK=true
# Preferred chat provider when the frontend does not specify one
# Accepts: openai | deepseek | gemini | vertex | grok
DEFAULT_CHAT_PROVIDER=grok

# OpenAI
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
EMBEDDING_MODEL=text-embedding-3-large

# DeepSeek
DEEPSEEK_API_BASE=https://api.deepseek.com/v1
DEEPSEEK_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
DEEPSEEK_CHAT_MODEL=deepseek-chat
DEEPSEEK_EMBEDDING_MODEL=deepseek-embedder

# Gemini (Google Generative Language API)
# If using Gemini from Google AI Studio/Generative Language API
GEMINI_API_BASE=https://generativelanguage.googleapis.com/v1beta
GEMINI_API_KEY=AIza-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
GEMINI_CHAT_MODEL=learnlm-2.0-flash-experimental
#GEMINI_CHAT_MODEL=gemini-flash-latest
# Enable server-side context caching for large system prompts
GEMINI_ENABLE_CONTEXT_CACHE=true
# Cache TTL for server-side cached content (seconds)
GEMINI_CACHE_TTL_SECONDS=1800
# Minimum size (characters) of system/context text to attempt caching
GEMINI_CACHE_MIN_CHARS=4000
GEMINI_MAX_TOKENS=8192
GCP_LOCATION=us-east1
GEMINI_PROJECT_NAME=projects/213082693271
GEMINI_PROJECT_NUMBER=213082693271

# Frontend default LLM
NEXT_PUBLIC_DEFAULT_LLM=grok
#NEXT_PUBLIC_DEFAULT_LLM=gpt     # gpt | deepseek | gemini | vertex | grok
#NEXT_PUBLIC_DEFAULT_LLM=gemini
#NEXT_PUBLIC_DEFAULT_LLM=vertex
#NEXT_PUBLIC_DEFAULT_LLM=grok

# ---------------- Vertex AI (OAuth) ----------------
# Use Google Application Default Credentials in the backend container
VERTEX_PROJECT_ID=your-gcp-project-id
VERTEX_LOCATION=us-east1
VERTEX_PUBLISHER=google
VERTEX_CHAT_MODEL=gemini-1.5-pro-002
VERTEX_ENABLE_CONTEXT_CACHE=true
VERTEX_CACHE_TTL_SECONDS=1800
VERTEX_CACHE_MIN_CHARS=4000
VERTEX_MAX_TOKENS=8192
VERTEX_TRUNCATE_SYSTEM_CHARS=60000
VERTEX_USE_OAUTH=true
VERTEX_CHAT_MAX_TOKENS=4096
# Increase if you see ReadTimeout on large generations
VERTEX_HTTP_TIMEOUT_SECONDS=300
# Enable only if you want input token counts (adds latency)
VERTEX_ENABLE_COUNT_TOKENS=false
# If using a service account key, mount and point this path
GOOGLE_APPLICATION_CREDENTIALS=/secrets/gcp.json

# ---------------- xAI Grok ----------------
# xAI Grok API (OpenAI-compatible)
GROK_API_BASE=https://api.x.ai/v1
GROK_API_KEY=xai-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
GROK_CHAT_MODEL=grok-4-fast-reasoning
# Max output tokens per call (optimize for large context; Grok-3 supports large windows)
GROK_MAX_TOKENS=16384
# Context window tokens (approximate; used for safety calculations)
GROK_CONTEXT_WINDOW=2000000
# Enable context caching if xAI adds support (placeholder)
GROK_ENABLE_CONTEXT_CACHE=false
GROK_CACHE_TTL_SECONDS=1800
GROK_CACHE_MIN_CHARS=4000

# LEGACY
# Alembic has been removed; these toggles are ignored.
AUTOGEN_MIGRATIONS_DEV=false
AUTOGEN_MIGRATIONS_PROD=false

# NEW V2
# Internal URL for frontend server runtime to reach backend via Docker network
# Browser clients should continue to use NEXT_PUBLIC_API_BASE
BACKEND_INTERNAL_URL=http://backend:8000

# NEW V2
# Default administrator account used to bootstrap on startup.
# Username uses email.
ADMIN_EMAIL=admin@example.com
ADMIN_PASSWORD=ChangeMe_12345
ADMIN_FULL_NAME=Administrator

# NEW V2 dbrag vector store (Qdrant)
DBRAG_QDRANT_URL=http://dbrag:6333
# Optionally enable gRPC by setting a URL (e.g. grpc://dbrag:6334); leave blank to disable.
DBRAG_QDRANT_GRPC_URL=
# Leave blank unless you enable Qdrant API keys.
DBRAG_QDRANT_API_KEY=
DBRAG_QDRANT_TIMEOUT_SECONDS=120
DBRAG_QDRANT_HTTP_PORT=6333
DBRAG_QDRANT_GRPC_PORT=6334
DBRAG_QDRANT_CLUSTER_ENABLED=false
DBRAG_QDRANT_DISABLE_TELEMETRY=true

# NEW V2 RAG ingestion defaults
# Default container mount; override with an absolute host path when running outside Docker.
RAG_DOCUMENT_ROOT=/repo/DOCS
RAG_ALLOWED_EXTENSIONS=.pdf,.docx,.doc,.txt,.pptx,.ppt,.md
RAG_CHUNK_SIZE=1000
RAG_CHUNK_OVERLAP=200
RAG_EMBEDDING_MODEL=ai/llama3.1:8B-Q4_K_M
RAG_EMBEDDING_DIMENSION=3072
RAG_MAX_BATCH_SIZE=16
QDRANT_UPSERT_BATCH_SIZE=128

# NEW V2 GPU passthrough
NVIDIA_VISIBLE_DEVICES=all
NVIDIA_DRIVER_CAPABILITIES=compute,utility
NVIDIA_GPU_COUNT=-1

# DONT REMOVE THIS COMMENT:
# Generate token here
# https://huggingface.co/settings/tokens
HF_TOKEN=xxxxxxxxxxxxxxxxxxxxx
